{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined functions\n",
    "import odor_statistics_lib as osm\n",
    "\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "#suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.TimeSeries = pd.Series \n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import scipy.stats as st\n",
    "\n",
    "#classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#plots\n",
    "import string\n",
    "import figurefirst\n",
    "from figurefirst import FigureLayout,mpl_functions\n",
    "import matplotlib.ticker as mtick\n",
    "import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # for colorbar\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '~/Documents/Myfiles/DataAnalysis/data/Sprints/HighRes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf_windy = pd.read_hdf(dir+'Windy/WindyStats.h5')\n",
    "fdf_notwindy = pd.read_hdf(dir+'NotWindy/NotWindyStats.h5')\n",
    "fdf_forest = pd.read_hdf(dir+'Forest/ForestStats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc=pd.read_hdf(dir+'Classifier/accuracylwsh.h5')\n",
    "# score=pd.read_hdf(dir+'Classifier/Scoreslwsh.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Classwise Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.drop(list(acc.filter(regex = 'feature')), axis = 1, inplace = True)\n",
    "feature_number=np.arange(1,51,1)\n",
    "confidence_interval_1=[]\n",
    "confidence_interval_2=[]\n",
    "confidence_interval_3=[]\n",
    "median_1=[]\n",
    "median_2=[]\n",
    "median_3=[]\n",
    "for i in range(0,len(acc.columns),3):\n",
    "    confidence_interval_1.append(st.t.interval(alpha=0.95, \n",
    "                                              df=len(acc)-1, loc=np.mean(acc.iloc[:,i]), \n",
    "                                              scale=st.sem(acc.iloc[:,i]))) \n",
    "    median_1.append(np.median(acc.iloc[:,i]))\n",
    "\n",
    "    confidence_interval_2.append(st.t.interval(alpha=0.95, \n",
    "                                              df=len(acc)-1, loc=np.mean(acc.iloc[:,i+1]), \n",
    "                                              scale=st.sem(acc.iloc[:,i+1]))) \n",
    "    median_2.append(np.median(acc.iloc[:,i+1]))\n",
    "    confidence_interval_3.append(st.t.interval(alpha=0.95, \n",
    "                                              df=len(acc)-1, loc=np.mean(acc.iloc[:,i+2]), \n",
    "                                              scale=st.sem(acc.iloc[:,i+2], nan_policy='omit'))) \n",
    "    median_3.append(np.median(acc.iloc[:,i+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## storing in a dataframe\n",
    "df_interval = pd.DataFrame({'cl_1_int_1': np.array(confidence_interval_1)[:,0],\n",
    "                        'cl_1_int_2': np.array(confidence_interval_1)[:,1],\n",
    "                        'cl_2_int_1': np.array(confidence_interval_2)[:,0],\n",
    "                        'cl_2_int_2': np.array(confidence_interval_2)[:,1],\n",
    "                        'cl_3_int_1': np.array(confidence_interval_3)[:,0],\n",
    "                        'cl_3_int_2': np.array(confidence_interval_3)[:,1],\n",
    "                        'median_cl_1':median_1,\n",
    "                        'median_cl_2':median_2,\n",
    "                        'median_cl_3':median_3,\n",
    "                        'feature':feature_number})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolating NaNs\n",
    "# df_interval.cl_3_int_1 = df_interval.cl_3_int_1.interpolate(method='nearest')\n",
    "# df_interval.cl_3_int_2 = df_interval.cl_3_int_2.interpolate(method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f,ax=plt.subplots(1,1,figsize=(8,6))\n",
    "# ax.grid(False)\n",
    "# ax.scatter(df_interval.feature, df_interval.median_cl_1, label='Median Class 1')\n",
    "# ax.scatter(df_interval.feature, df_interval.median_cl_2, label='Median Class 2')\n",
    "# ax.scatter(df_interval.feature, df_interval.median_cl_3, label='Median Class 3')\n",
    "\n",
    "# ax.fill_between(df_interval.feature, df_interval['cl_1_int_1'], df_interval['cl_1_int_2'],\n",
    "#                 where=df_interval['cl_1_int_2'] >= df_interval['cl_1_int_1'],\n",
    "#                 facecolor='blue', alpha=0.2, interpolate=True)\n",
    "\n",
    "# ax.fill_between(df_interval.feature, df_interval['cl_2_int_1'], df_interval['cl_2_int_2'],\n",
    "#                 where=df_interval['cl_2_int_2'] >= df_interval['cl_2_int_1'],\n",
    "#                 facecolor='blue', alpha=0.2, interpolate=True)\n",
    "\n",
    "# ax.fill_between(df_interval.feature, df_interval['cl_3_int_1'], df_interval['cl_3_int_2'],\n",
    "#                 where=df_interval['cl_3_int_2'] >= df_interval['cl_3_int_1'],\n",
    "#                 facecolor='blue', alpha=0.2, interpolate=True)\n",
    "# ax.set_xlabel('N_Features')\n",
    "# ax.set_ylabel('% Class Accuracy')\n",
    "\n",
    "# mpl_functions.adjust_spines(ax,['left','bottom'],spine_locations={}, \n",
    "#                                 smart_bounds=True,\n",
    "#                                 xticks=[0,10,20,30,40,50],\n",
    "#                                 yticks=[0,0.5,1],\n",
    "#                                 linewidth=1)\n",
    "\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# ax.set_title('LWS tested on HWS')\n",
    "# figurefirst.mpl_functions.set_fontsize(f, 16)\n",
    "\n",
    "# f.tight_layout(pad=2)\n",
    "# # f.savefig('../../Figure/Accuracylwsh.jpeg', dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_column_forest(dataframe):\n",
    "    dataframe.loc[dataframe.avg_dist_from_source < 5, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.avg_dist_from_source >= 5)  & (dataframe.avg_dist_from_source < 10), 'type'] = 1\n",
    "    dataframe.loc[dataframe.avg_dist_from_source >= 10, 'type'] = 2\n",
    "    return dataframe\n",
    "\n",
    "def create_class_column(dataframe):\n",
    "    dataframe.loc[dataframe.avg_dist_from_source < 5, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.avg_dist_from_source >= 5)  & (dataframe.avg_dist_from_source < 30), 'type'] = 1\n",
    "    dataframe.loc[dataframe.avg_dist_from_source >= 30, 'type'] = 2\n",
    "    return dataframe\n",
    "\n",
    "def create_class_column_log(dataframe):\n",
    "    dataframe.loc[dataframe.log_avg_dist_from_source_signed < 0.7, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.log_avg_dist_from_source_signed >= 0.7)  & \n",
    "                  (dataframe.log_avg_dist_from_source_signed < 1.5), 'type'] = 1\n",
    "    dataframe.loc[dataframe.log_avg_dist_from_source_signed >= 1.5, 'type'] = 2\n",
    "    return dataframe\n",
    "\n",
    "def check_length(dataframe, Nrows, nrows,N):\n",
    "    if (len(Nrows) !=N):\n",
    "        rowsneeded  = N - len(Nrows) \n",
    "        Nrows = Nrows.append(dataframe[(nrows.index-rowsneeded).values[0]:(nrows.index).values[0]])\n",
    "        Nrows = Nrows.sort_index()\n",
    "        return Nrows\n",
    "    else:\n",
    "        return Nrows\n",
    "    \n",
    "def get_rows(dataframe, N):\n",
    "    nrows = dataframe.sample(1)\n",
    "    Nrows = dataframe[(nrows.index).values[0]:(nrows.index+N).values[0]]\n",
    "    newrows = check_length(dataframe,Nrows, nrows, N)\n",
    "    return newrows\n",
    "\n",
    "# for each collection of data to use for the classifier, get statistics from N encounters\n",
    "def get_N_consecutive_encounter_stats(dataframe, distance_class, N):\n",
    "    df_q = dataframe.query('type == ' + str(distance_class))   \n",
    "    df_q.reset_index(inplace=True, drop=True)     \n",
    "    Nrows = get_rows(df_q,N)\n",
    "    \n",
    "    return np.ravel( Nrows[['mean_concentration','mean_ef','log_whiff','mean_ma']].values )\n",
    "\n",
    "\n",
    "# for each collection of data to use for the classifier, get statistics from N encounters\n",
    "def get_N_random_encounter_stats(dataframe, distance_class, N):\n",
    "    df_q = dataframe.query('type == ' + str(distance_class))   \n",
    "    df_q.reset_index(inplace=True, drop=True)     \n",
    "    Nrows = df_q.sample(N)\n",
    "    return np.ravel( Nrows[['mean_concentration','mean_ef','log_whiff','mean_ma']].values )\n",
    "\n",
    "\n",
    "def gather_stat_random(dataframe, distance_class, number_of_encounters,X,y):\n",
    "    for i in range(100):\n",
    "        X.append(get_N_random_encounter_stats(dataframe, distance_class, number_of_encounters))\n",
    "        y.append(distance_class)\n",
    "    return X,y\n",
    "\n",
    "def gather_stat_consecutive(dataframe, distance_class, number_of_encounters,X,y):\n",
    "    for i in range(2000):\n",
    "        X.append(get_N_consecutive_encounter_stats(dataframe, distance_class, number_of_encounters))\n",
    "        y.append(distance_class)\n",
    "    return X,y\n",
    "\n",
    "def stack_arrays(a):\n",
    "    A = np.full((len(a), max(map(len, a))), np.nan)\n",
    "    for i, aa in enumerate(a):\n",
    "        A[i, :len(aa)] = aa\n",
    "    return A\n",
    "\n",
    "# def class_population_accuracy(ytest,y_pred):\n",
    "    \n",
    "#     cm = confusion_matrix(ytest, y_pred)\n",
    "#     return ((cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]).diagonal())\n",
    "\n",
    "def class_population_accuracy(ytest,y_pred):\n",
    "    cm = confusion_matrix(ytest, y_pred)\n",
    "    class_acc=[]\n",
    "    # Calculate the accuracy for each one of our classes\n",
    "    for idx, cls in enumerate([0,1,2]):\n",
    "        # True negatives are all the samples that are not our current GT class (not the current row) \n",
    "        # and were not predicted as the current class (not the current column)\n",
    "        tn = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "        # True positives are all the samples of our current GT class that were predicted as such\n",
    "        tp = cm[idx, idx]\n",
    "        # The accuracy for the current class is ratio between correct predictions to all predictions\n",
    "        class_acc.append((tp+tn)/np.sum(cm))\n",
    "    return (class_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "number_of_encounters = 15\n",
    "trainset= fdf_notwindy\n",
    "testset = fdf_windy\n",
    "\n",
    "for i in range (0,5): #bootstrapping\n",
    "    Xtest = []\n",
    "    ytest = []\n",
    "    Xtrain = []\n",
    "    ytrain = []\n",
    "    \n",
    "    for distance_class in [0,1,2]:\n",
    "        Xtrain,ytrain = gather_stat_random(trainset,distance_class,number_of_encounters, Xtrain,ytrain) \n",
    "    Xtrain = stack_arrays(Xtrain)\n",
    "\n",
    "\n",
    "    for distance_class in [0,1,2]:\n",
    "        Xtest,ytest = gather_stat_random(testset,distance_class,number_of_encounters, Xtest,ytest)    \n",
    "    Xtest = stack_arrays(Xtest)\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    y_pred = clf.fit(Xtrain,ytrain).predict(Xtest)\n",
    "    \n",
    "    accuracy.append(class_population_accuracy(ytest,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8266666666666667, 0.7533333333333333, 0.92],\n",
       " [0.83, 0.7233333333333334, 0.88],\n",
       " [0.7933333333333333, 0.7366666666666667, 0.9166666666666666],\n",
       " [0.81, 0.6966666666666667, 0.8733333333333333],\n",
       " [0.8133333333333334, 0.71, 0.89]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8043333333333333, 0.6926666666666667, 0.8843333333333333],\n",
       " [0.8366666666666667, 0.732, 0.8926666666666667],\n",
       " [0.7996666666666666, 0.7126666666666667, 0.9056666666666666],\n",
       " [0.7806666666666666, 0.6813333333333333, 0.8933333333333333],\n",
       " [0.7853333333333333, 0.6843333333333333, 0.889]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "accdf=pd.DataFrame()\n",
    "accdf['class_0']=[item[0] for item in accuracy]\n",
    "accdf['class_1']=[item[1] for item in accuracy]\n",
    "accdf['class_2']=[item[2] for item in accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test set Score:  0.7183333333333334\n",
      "Number of mislabeled points out of a total 600 points : 169\n"
     ]
    }
   ],
   "source": [
    "## TRAINED WITH NOT WINDY AND TESTED on WINDY\n",
    "clf = GaussianNB()\n",
    "y_pred = clf.fit(Xtrain,ytrain).predict(Xtest)\n",
    "print(\"Naive Bayes Test set Score: \",clf.score(Xtest, ytest))\n",
    "\n",
    "# # print(\"Naive Bayes Train set Score: \",clf.score(Xtrain, ytrain))\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (Xtest.shape[0], (ytest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7183333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(ytest, y_pred))\n",
    "# print(metrics.classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_descent(gradient, start, learn_rate, n_iter=50, tolerance=1e-06):\n",
    "#     vector = start\n",
    "   \n",
    "#     for _ in range(n_iter):\n",
    "#         diff = -learn_rate * gradient(vector)\n",
    "#         if np.all(np.abs(diff) <= tolerance):\n",
    "#             break\n",
    "#         vector += diff\n",
    "#         a.append(vector)\n",
    "#     return vector\n",
    "\n",
    "\n",
    "# a = []\n",
    "# vector = gradient_descent(\n",
    "#     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,\n",
    "#     learn_rate=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# def f(x):\n",
    "# #     return np.sin(x) + x + x * np.sin(x)\n",
    "#     return 4 * np.power(x,3) - 10*x -3\\\n",
    "\n",
    "\n",
    "# x = np.linspace(-3, 3, 50)\n",
    "# plt.plot(x,f(x))\n",
    "# plt.plot(a,'o')\n",
    "# plt.xlim(-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model theoretical whiff frequency as a gamma distribution with a scale factor that increases with distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiff_freq_from_distance(distance, N):\n",
    "    scale = 1 + 4*np.log(distance+1)\n",
    "    G = scipy.stats.gamma(2, 0, scale)\n",
    "    raw_whiff_frequencies = G.rvs(N)\n",
    "    return raw_whiff_frequencies\n",
    "\n",
    "distance = np.logspace(-1, 3)\n",
    "\n",
    "whiff_freqs = []\n",
    "distances = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    wfs = whiff_freq_from_distance(d, N)\n",
    "    whiff_freqs.extend( wfs.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( np.array(distances), np.array(whiff_freqs), '.', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With increasing distance, concentration falls, whiffs harder to detect, so frequency should fall\n",
    "\n",
    "Model concentration effect as a normal distribution with a mean that falls according to an exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concentration_effect_from_distance(distance, N):\n",
    "    mean = scipy.stats.expon(0,40).pdf(d) / 0.025\n",
    "    concentration_effects = scipy.stats.norm(mean, 0.3).rvs(N)\n",
    "    concentration_effects[concentration_effects<0] = 0\n",
    "    return concentration_effects\n",
    "\n",
    "distance = np.logspace(-1, 3)\n",
    "\n",
    "conc_effects = []\n",
    "distances = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    ces = concentration_effect_from_distance(d, N)\n",
    "    conc_effects.extend( ces.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( np.array(distances), np.array(conc_effects), '.', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model observed whiff frequency as a product of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiff_freqs = []\n",
    "distances = []\n",
    "concentrations = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    \n",
    "    raw_whiff_frequencies = whiff_freq_from_distance(d, N)\n",
    "    concentration_effects = concentration_effect_from_distance(d, N)\n",
    "    \n",
    "    wfs = raw_whiff_frequencies*concentration_effects\n",
    "    wfs = wfs / 10\n",
    "    \n",
    "    whiff_freqs.extend( wfs.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    concentrations.extend(concentration_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot( np.array(distances), np.array(whiff_freqs), '.', alpha=0.3)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)\n",
    "ax.set_ylim(0, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'distance': distances, \n",
    "                       'whiff_freq': whiff_freqs, \n",
    "                       'concentration': concentrations,\n",
    "                       'type': [0]*len(whiff_freqs)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
