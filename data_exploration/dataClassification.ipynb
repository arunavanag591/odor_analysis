{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined functions\n",
    "import odor_statistics_lib as osm\n",
    "\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "#suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.TimeSeries = pd.Series \n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "\n",
    "#classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#plots\n",
    "import string\n",
    "import figurefirst\n",
    "from figurefirst import FigureLayout,mpl_functions\n",
    "import matplotlib.ticker as mtick\n",
    "import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # for colorbar\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '~/DataAnalysis/data/Sprints/HighRes/'\n",
    "fdf_windy = pd.read_hdf(dir+'Windy/WindyStatsLog.h5')\n",
    "fdf_notwindy = pd.read_hdf(dir+'NotWindy/NotWindyStats.h5')\n",
    "fdf_forest = pd.read_hdf(dir+'Forest/ForestStats.h5')\n",
    "\n",
    "# windy = pd.read_hdf(dir+'Windy/WindyMASigned.h5')\n",
    "# nwindy = pd.read_hdf(dir+'NotWindy/NotWindyMASigned.h5')\n",
    "# forest = pd.read_hdf(dir+'Forest/ForestMASigned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_class_column(fdf_notwindy)\n",
    "# create_class_column(fdf_windy)\n",
    "# create_class_column(fdf_forest)\n",
    "# fdf_notwindy.to_hdf(dir+'NotWindy/NotWindyStats.h5', key='df', mode='w')\n",
    "# fdf_windy.to_hdf(dir+'Windy/WindyStats.h5', key='df', mode='w')\n",
    "# fdf_forest.to_hdf(dir+'Forest/ForestStats.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir1 = 'AccuracyScoresNB/Random/'\n",
    "# dir2 = 'AccuracyScoresNB/Consecutive/'\n",
    "# dir = '~/DataAnalysis/data/Sprints/HighRes/'\n",
    "# scorelh = pd.read_hdf(dir+ dir1+ 'Logged/Scoreslwsh.h5')\n",
    "# scorehl = pd.read_hdf(dir+ dir1+ 'Logged/Scoreshwsl.h5')\n",
    "# scorenw = pd.read_hdf(dir+ dir1+ 'Logged/Scores_desert_nwindy.h5')\n",
    "# scorew = pd.read_hdf(dir+ dir1+ 'Logged/Scores_desert_windy.h5')\n",
    "# scoref = pd.read_hdf(dir+ dir1+ 'Logged/Scores_desert_forest.h5')\n",
    "# scoreft = pd.read_hdf(dir+ dir1+ 'NotLogged/Scoresf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ticks():\n",
    "#     for tick in ax.xaxis.get_major_ticks():\n",
    "#         tick.tick1line.set_visible(True)\n",
    "\n",
    "#     for tick in ax.yaxis.get_major_ticks():\n",
    "#         tick.tick1line.set_visible(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows=1\n",
    "# ncols=1\n",
    "# f , (ax) = plt.subplots(nrows,ncols, figsize = (8,4))\n",
    "# # f.text(0.5, 0.008, 'No. of Whiffs per Feature', ha='center', fontweight='bold')\n",
    "# # f.text(0.001, 0.5, 'Accuracy w 3 Classes', va='center', rotation='vertical',fontweight='bold')\n",
    "# ax.scatter(scorelh.encounters, scorelh.accuracy, label='LWS Tested on HWS')\n",
    "# ax.scatter(scorehl.encounters, scorehl.accuracy, label='HWS Tested on LWS')\n",
    "# ax.scatter(scorew.encounters, scorew.accuracy, label='Desert Tested on HWS')\n",
    "# ax.scatter(scorenw.encounters, scorenw.accuracy,label='Desert Tested on LWS')\n",
    "# ax.scatter(scoref.encounters, scoref.accuracy, label='Desert Tested on Forest')\n",
    "\n",
    "# ax.grid(False)\n",
    "# ax.set_xlabel('Number of Whiff Statistics/Feature')\n",
    "# ax.set_ylabel('Accuracy')\n",
    "# ax.set_title('Naive Bayes Classification on 3 classes')\n",
    "# mpl_functions.adjust_spines(ax,['left','bottom'],spine_locations={}, \n",
    "#                                 smart_bounds=True,\n",
    "#                                 xticks=[0,10,20,30,40,50],\n",
    "#                                 yticks=[0.3,0.8,1],\n",
    "#                                 linewidth=1)\n",
    "\n",
    "\n",
    "# figurefirst.mpl_functions.set_fontsize(f, 16)\n",
    "\n",
    "\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# # Put a legend to the right of the current axis\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=14)\n",
    "\n",
    "# f.tight_layout(pad=2)\n",
    "\n",
    "# # f.savefig('../../Figure/GNBResults.jpeg', dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows=3\n",
    "# ncols=2\n",
    "# f , (ax) = plt.subplots(nrows,ncols, figsize = (10,10))\n",
    "# f.text(0.5, 0.008, 'No. of Whiffs per Feature', ha='center', fontweight='bold')\n",
    "# f.text(0.001, 0.5, 'Accuracy w 3 Classes', va='center', rotation='vertical',fontweight='bold')\n",
    "# ax[0][0].scatter(scorelh.encounters, scorelh.accuracy)\n",
    "# ax[0][0].set_title('LWS Tested on HWS')\n",
    "\n",
    "# ax[0][1].scatter(scorehl.encounters, scorehl.accuracy)\n",
    "# ax[0][1].set_title('HWS Tested on LWS')\n",
    "\n",
    "# ax[1][0].scatter(scorew.encounters, scorew.accuracy)\n",
    "# ax[1][0].set_title('Desert Tested on HWS') \n",
    "\n",
    "# ax[1][1].scatter(scorenw.encounters, scorenw.accuracy)\n",
    "# ax[1][1].set_title('Desert Tested on LWS') \n",
    "\n",
    "# ax[2][0].scatter(scoref.encounters, scoref.accuracy)\n",
    "# ax[2][0].set_title('Desert Tested on Forest') \n",
    "\n",
    "# # ax[2][1].scatter(scoreft.encounters, scoreft.accuracy)\n",
    "# # ax[2][1].set_title('Desert Tested on Forest') \n",
    "\n",
    "\n",
    "# f.delaxes(ax[2,1]) ## Deleted the unwanted row\n",
    "# for i in range(nrows):\n",
    "#     for j in range (ncols):\n",
    "#         ax[i][j].grid(False)\n",
    "        \n",
    "# f.tight_layout(pad=2)\n",
    "# f.suptitle('Naive Bayes Results w 1500 features',fontweight='bold')\n",
    "# figurefirst.mpl_functions.set_fontsize(f, 16)\n",
    "# # f.savefig('../../Figure/GNBResultsLogged.jpeg', dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(df,index,fdf):\n",
    "    osm.avg_distance(df,index,fdf)\n",
    "    osm.mean_conc(df,index,fdf)\n",
    "    osm.motion_statistics(df,index,fdf)\n",
    "    osm.whiff_blank_duration(df,index,fdf)\n",
    "    osm.trajectory_speed(df,index,fdf)\n",
    "    osm.encounter_frequency(df,index,fdf,1,2)\n",
    "    osm.mean_avg(df,index,fdf)\n",
    "    osm.mean_conc(df,index,fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide = 'ignore') \n",
    "indexW = osm.get_index(windy)\n",
    "indexNW = osm.get_index(nwindy)\n",
    "fdf_windy = pd.DataFrame()\n",
    "fdf_notwindy = pd.DataFrame()\n",
    "fdf_forest = pd.DataFrame()\n",
    "get_statistics(windy,indexW,fdf_windy)\n",
    "get_statistics(nwindy,indexNW,fdf_notwindy)\n",
    "indexF = osm.get_index_forest(forest)\n",
    "get_statistics(forest,indexF,fdf_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf_windy['type']=np.zeros(len(fdf_windy))\n",
    "fdf_notwindy['type']=np.zeros(len(fdf_notwindy))\n",
    "fdf_forest['type']=np.zeros(len(fdf_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdf_forest.to_hdf(dir+'Forest/ForestStats.h5', key='fdf_forest', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.011872336272722\n",
      "31.622776601683793\n",
      "50.11872336272722\n"
     ]
    }
   ],
   "source": [
    "print(np.power(10,0.7)) # 5meters\n",
    "# print(np.power(10,1.31))\n",
    "print(np.power(10,1.5))\n",
    "print(np.power(10,1.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_column_forest(dataframe):\n",
    "    dataframe.loc[dataframe.avg_dist_from_source < 5, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.avg_dist_from_source >= 5)  & (dataframe.avg_dist_from_source < 10), 'type'] = 1\n",
    "    dataframe.loc[dataframe.avg_dist_from_source >= 10, 'type'] = 2\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_column(dataframe):\n",
    "    dataframe.loc[dataframe.avg_dist_from_source < 5, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.avg_dist_from_source >= 5)  & (dataframe.avg_dist_from_source < 30), 'type'] = 1\n",
    "    dataframe.loc[dataframe.avg_dist_from_source >= 30, 'type'] = 2\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_column_log(dataframe):\n",
    "    dataframe.loc[dataframe.log_avg_dist_from_source_signed < 0.7, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.log_avg_dist_from_source_signed >= 0.7)  & \n",
    "                  (dataframe.log_avg_dist_from_source_signed < 1.5), 'type'] = 1\n",
    "    dataframe.loc[dataframe.log_avg_dist_from_source_signed >= 1.5, 'type'] = 2\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "check_length() missing 1 required positional argument: 'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_N_consecutive_encounter_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfdf_windy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36mget_N_consecutive_encounter_stats\u001b[0;34m(dataframe, distance_class, N)\u001b[0m\n\u001b[1;32m      3\u001b[0m df_q \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype == \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(distance_class))   \n\u001b[1;32m      4\u001b[0m df_q\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)     \n\u001b[0;32m----> 5\u001b[0m Nrows \u001b[38;5;241m=\u001b[39m \u001b[43mget_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m Nrows \u001b[38;5;241m=\u001b[39m check_length(dataframe,N)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel( Nrows[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_concentration\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_ef\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_whiff\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_ma\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues )\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mget_rows\u001b[0;34m(dataframe, N)\u001b[0m\n\u001b[1;32m      2\u001b[0m nrows \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m Nrows \u001b[38;5;241m=\u001b[39m dataframe[(nrows\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]:(nrows\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m+\u001b[39mN)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m----> 4\u001b[0m Nrows \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Nrows\n",
      "\u001b[0;31mTypeError\u001b[0m: check_length() missing 1 required positional argument: 'N'"
     ]
    }
   ],
   "source": [
    "get_N_consecutive_encounter_stats(fdf_windy, 0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length(dataframe, Nrows, N):\n",
    "    if (len(Nrows) !=N):\n",
    "        rowsneeded  = N - len(Nrows) \n",
    "        Nrows.append(dataframe[(nrows.index-rowsneeded).values[0]:(nrows.index-1).values[0]])\n",
    "#         check_length(dataframe,Nrows, N)\n",
    "    else:\n",
    "        return Nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(dataframe, N):\n",
    "    nrows = dataframe.sample(1)\n",
    "    Nrows = dataframe[(nrows.index).values[0]:(nrows.index+N).values[0]]\n",
    "    Nrows = check_length(dataframe,Nrows, N)\n",
    "    return Nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each collection of data to use for the classifier, get statistics from N encounters\n",
    "def get_N_consecutive_encounter_stats(dataframe, distance_class, N):\n",
    "    df_q = dataframe.query('type == ' + str(distance_class))   \n",
    "    df_q.reset_index(inplace=True, drop=True)     \n",
    "    Nrows = get_rows(df_q,N)\n",
    "    \n",
    "    return np.ravel( Nrows[['mean_concentration','mean_ef','log_whiff','mean_ma']].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each collection of data to use for the classifier, get statistics from N encounters\n",
    "def get_N_random_encounter_stats(dataframe, distance_class, N):\n",
    "    df_q = dataframe.query('type == ' + str(distance_class))   \n",
    "    df_q.reset_index(inplace=True, drop=True)     \n",
    "    Nrows = df_q.sample(N)\n",
    "    return np.ravel( Nrows[['mean_concentration','mean_ef','log_whiff','mean_ma']].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_arrays(a):\n",
    "    A = np.full((len(a), max(map(len, a))), np.nan)\n",
    "    for i, aa in enumerate(a):\n",
    "        A[i, :len(aa)] = aa\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_stat_random(dataframe, distance_class, number_of_encounters,X,y):\n",
    "    for i in range(2000):\n",
    "        X.append(get_N_random_encounter_stats(dataframe, distance_class, number_of_encounters))\n",
    "        y.append(distance_class)\n",
    "    return X,y\n",
    "\n",
    "def gather_stat_consecutive(dataframe, distance_class, number_of_encounters,X,y):\n",
    "    for i in range(2000):\n",
    "        X.append(get_N_consecutive_encounter_stats(dataframe, distance_class, number_of_encounters))\n",
    "        y.append(distance_class)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest  = pd.concat([fdf_notwindy, fdf_windy])\n",
    "newtest.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Xtest = []\n",
    "    ytest = []\n",
    "    Xtrain = []\n",
    "    ytrain = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl = [0,1,2]\n",
    "number_of_encounters = 15\n",
    "\n",
    "## Trainset\n",
    "trainset= fdf_notwindy\n",
    "Xtrain = []\n",
    "ytrain = []\n",
    "\n",
    "## TestSet\n",
    "testset = fdf_windy\n",
    "Xtest = []\n",
    "ytest = []\n",
    "\n",
    "for distance_class in [0,1,2]:\n",
    "    Xtrain,ytrain = gather_stat_consecutive(trainset,distance_class,number_of_encounters, Xtrain,ytrain) \n",
    "Xtrain = stack_arrays(Xtrain)\n",
    "# # Xtrain = np.vstack(Xtrain)\n",
    "\n",
    "\n",
    "# for distance_class in [0,1,2]:\n",
    "#     Xtest,ytest = gather_stat_consecutive(testset,distance_class,number_of_encounters, Xtest,ytest)    \n",
    "# Xtest = stack_arrays(Xtest)\n",
    "# Xtest = np.vstack(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test set Score:  0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "## TRAINED WITH NOT WINDY AND TESTED on WINDY\n",
    "clf = GaussianNB()\n",
    "y_pred = clf.fit(Xtrain,ytrain).predict_proba(Xtest)\n",
    "print(\"Naive Bayes Test set Score: \",clf.score(Xtest, ytest))\n",
    "\n",
    "# # print(\"Naive Bayes Train set Score: \",clf.score(Xtrain, ytrain))\n",
    "# print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "#       % (Xtest.shape[0], (ytest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6948333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.97      0.76      2000\n",
      "           1       0.78      0.14      0.23      2000\n",
      "           2       0.76      0.98      0.86      2000\n",
      "\n",
      "    accuracy                           0.69      6000\n",
      "   macro avg       0.72      0.69      0.62      6000\n",
      "weighted avg       0.72      0.69      0.62      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(ytest, y_pred))\n",
    "print(metrics.classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain[np.isnan(Xtrain)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 3000 points : 1137\n",
      "Naive Bayes score:  0.621\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# test classifier\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "print(\"Naive Bayes score: \",gnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_descent(gradient, start, learn_rate, n_iter=50, tolerance=1e-06):\n",
    "#     vector = start\n",
    "   \n",
    "#     for _ in range(n_iter):\n",
    "#         diff = -learn_rate * gradient(vector)\n",
    "#         if np.all(np.abs(diff) <= tolerance):\n",
    "#             break\n",
    "#         vector += diff\n",
    "#         a.append(vector)\n",
    "#     return vector\n",
    "\n",
    "\n",
    "# a = []\n",
    "# vector = gradient_descent(\n",
    "#     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,\n",
    "#     learn_rate=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# def f(x):\n",
    "# #     return np.sin(x) + x + x * np.sin(x)\n",
    "#     return 4 * np.power(x,3) - 10*x -3\\\n",
    "\n",
    "\n",
    "# x = np.linspace(-3, 3, 50)\n",
    "# plt.plot(x,f(x))\n",
    "# plt.plot(a,'o')\n",
    "# plt.xlim(-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model theoretical whiff frequency as a gamma distribution with a scale factor that increases with distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 60)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiff_freq_from_distance(distance, N):\n",
    "    scale = 1 + 4*np.log(distance+1)\n",
    "    G = scipy.stats.gamma(2, 0, scale)\n",
    "    raw_whiff_frequencies = G.rvs(N)\n",
    "    return raw_whiff_frequencies\n",
    "\n",
    "distance = np.logspace(-1, 3)\n",
    "\n",
    "whiff_freqs = []\n",
    "distances = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    wfs = whiff_freq_from_distance(d, N)\n",
    "    whiff_freqs.extend( wfs.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( np.array(distances), np.array(whiff_freqs), '.', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With increasing distance, concentration falls, whiffs harder to detect, so frequency should fall\n",
    "\n",
    "Model concentration effect as a normal distribution with a mean that falls according to an exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concentration_effect_from_distance(distance, N):\n",
    "    mean = scipy.stats.expon(0,40).pdf(d) / 0.025\n",
    "    concentration_effects = scipy.stats.norm(mean, 0.3).rvs(N)\n",
    "    concentration_effects[concentration_effects<0] = 0\n",
    "    return concentration_effects\n",
    "\n",
    "distance = np.logspace(-1, 3)\n",
    "\n",
    "conc_effects = []\n",
    "distances = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    ces = concentration_effect_from_distance(d, N)\n",
    "    conc_effects.extend( ces.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( np.array(distances), np.array(conc_effects), '.', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model observed whiff frequency as a product of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiff_freqs = []\n",
    "distances = []\n",
    "concentrations = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    \n",
    "    raw_whiff_frequencies = whiff_freq_from_distance(d, N)\n",
    "    concentration_effects = concentration_effect_from_distance(d, N)\n",
    "    \n",
    "    wfs = raw_whiff_frequencies*concentration_effects\n",
    "    wfs = wfs / 10\n",
    "    \n",
    "    whiff_freqs.extend( wfs.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    concentrations.extend(concentration_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot( np.array(distances), np.array(whiff_freqs), '.', alpha=0.3)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)\n",
    "ax.set_ylim(0, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'distance': distances, \n",
    "                       'whiff_freq': whiff_freqs, \n",
    "                       'concentration': concentrations,\n",
    "                       'type': [0]*len(whiff_freqs)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
