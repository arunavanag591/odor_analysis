{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined functions\n",
    "import odor_statistics_lib as osm\n",
    "\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "#suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.TimeSeries = pd.Series \n",
    "\n",
    "#math\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "\n",
    "#classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#plots\n",
    "import string\n",
    "import figurefirst\n",
    "from figurefirst import FigureLayout,mpl_functions\n",
    "import matplotlib.ticker as mtick\n",
    "import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # for colorbar\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '~/Documents/Myfiles/DataAnalysis/data/Sprints/HighRes/'\n",
    "fdf_windy = pd.read_hdf(dir+'Windy/WindyStats.h5')\n",
    "fdf_notwindy = pd.read_hdf(dir+'NotWindy/NotWindyStats.h5')\n",
    "fdf_forest = pd.read_hdf(dir+'Forest/ForestStats.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_statistics(df,index,fdf):\n",
    "#     osm.avg_distance(df,index,fdf)\n",
    "#     osm.mean_conc(df,index,fdf)\n",
    "#     osm.motion_statistics(df,index,fdf)\n",
    "#     osm.whiff_blank_duration(df,index,fdf)\n",
    "#     osm.trajectory_speed(df,index,fdf)\n",
    "#     osm.encounter_frequency(df,index,fdf,1,2)\n",
    "#     osm.mean_avg(df,index,fdf)\n",
    "#     osm.mean_conc(df,index,fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(divide = 'ignore') \n",
    "# indexF = osm.get_index_forest(forest)\n",
    "# fdf_forest = pd.DataFrame()\n",
    "# get_statistics(forest,indexW,fdf_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(divide = 'ignore') \n",
    "# indexW = osm.get_index(windy)\n",
    "# indexNW = osm.get_index(nwindy)\n",
    "# fdf_windy = pd.DataFrame()\n",
    "# fdf_notwindy = pd.DataFrame()\n",
    "# get_statistics(windy,indexW,fdf_windy)\n",
    "# get_statistics(nwindy,indexNW,fdf_notwindy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdf_windy['type']=np.zeros(len(fdf_windy))\n",
    "# fdf_notwindy['type']=np.zeros(len(fdf_notwindy))\n",
    "# fdf_forest['type']=np.zeros(len(fdf_windy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdf_forest.to_hdf(dir+'Forest/ForestStats.h5', key='fdf_forest', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_column_forest(dataframe):\n",
    "    dataframe.loc[dataframe.avg_dist_from_source < 5, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.avg_dist_from_source >= 5)  & (dataframe.avg_dist_from_source < 10), 'type'] = 1\n",
    "    dataframe.loc[dataframe.avg_dist_from_source >= 10, 'type'] = 2\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_column(dataframe):\n",
    "    dataframe.loc[dataframe.avg_dist_from_source < 5, 'type'] = 0\n",
    "    dataframe.loc[(dataframe.avg_dist_from_source >= 5)  & (dataframe.avg_dist_from_source < 30), 'type'] = 1\n",
    "    # dataframe.loc[(dataframe.avg_dist_from_source >= 20) & (dataframe.avg_dist_from_source < 30), 'type'] = 2\n",
    "    dataframe.loc[dataframe.avg_dist_from_source >= 30, 'type'] = 2\n",
    "    # dataframe.loc[(dataframe.avg_dist_from_source >= 20) & (dataframe.avg_dist_from_source < 30), 'type'] = 3\n",
    "    # dataframe.loc[dataframe.avg_dist_from_source >= 30, 'type'] = 4\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each collection of data to use for the classifier, get statistics from N encounters\n",
    "def get_N_random_encounter_stats(dataframe, distance_class, N):\n",
    "    df_q = dataframe.query('type == ' + str(distance_class))\n",
    "    \n",
    "    Nrows = df_q.sample(N)\n",
    "    \n",
    "    return np.ravel( Nrows[['mean_concentration','mean_ef','log_whiff','mean_ma']].values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_stat(dataframe, distance_class, number_of_encounters,X,y):\n",
    "    for i in range(2000):\n",
    "        X.append(get_N_random_encounter_stats(dataframe, distance_class, number_of_encounters))\n",
    "        y.append(distance_class)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_class_column(fdf_notwindy)\n",
    "# create_class_column(fdf_windy)\n",
    "# create_class_column(fdf_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest  = pd.concat([fdf_notwindy, fdf_windy])\n",
    "newtest.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl = [0,1,2]\n",
    "number_of_encounters = 12\n",
    "\n",
    "## Trainset\n",
    "trainset= newtest\n",
    "Xtrain = []\n",
    "ytrain = []\n",
    "\n",
    "## TestSet\n",
    "testset = fdf_forest\n",
    "Xtest = []\n",
    "ytest = []\n",
    "\n",
    "for distance_class in [0,1,2]:\n",
    "    Xtrain,ytrain = gather_stat(trainset,distance_class,number_of_encounters, Xtrain,ytrain)    \n",
    "Xtrain = np.vstack(Xtrain)\n",
    "\n",
    "for distance_class in [0,1]:\n",
    "    Xtest,ytest = gather_stat(testset,distance_class,number_of_encounters, Xtest,ytest)    \n",
    "# Xtest = np.vstack(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_features</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.512315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.554774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_features  Accuracy\n",
       "0                   1  0.512315\n",
       "1                   2  0.554774"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pd.read_hdf(dir+'Scores.h5')\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test set Score:  0.49925\n",
      "Number of mislabeled points out of a total 4000 points : 2003\n"
     ]
    }
   ],
   "source": [
    "## TRAINED WITH NOT WINDY AND TESTED on WINDY\n",
    "clf = GaussianNB()\n",
    "y_pred = clf.fit(Xtrain,ytrain).predict(Xtest)\n",
    "print(\"Naive Bayes Test set Score: \",clf.score(Xtest, ytest))\n",
    "\n",
    "# # print(\"Naive Bayes Train set Score: \",clf.score(Xtrain, ytrain))\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (Xtest.shape[0], (ytest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(ytest, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 4263 points : 629\n",
      "Naive Bayes score:  0.8524513253577293\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtest, ytest, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# test classifier\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "print(\"Naive Bayes score: \",gnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_descent(gradient, start, learn_rate, n_iter=50, tolerance=1e-06):\n",
    "#     vector = start\n",
    "   \n",
    "#     for _ in range(n_iter):\n",
    "#         diff = -learn_rate * gradient(vector)\n",
    "#         if np.all(np.abs(diff) <= tolerance):\n",
    "#             break\n",
    "#         vector += diff\n",
    "#         a.append(vector)\n",
    "#     return vector\n",
    "\n",
    "\n",
    "# a = []\n",
    "# vector = gradient_descent(\n",
    "#     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,\n",
    "#     learn_rate=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# def f(x):\n",
    "# #     return np.sin(x) + x + x * np.sin(x)\n",
    "#     return 4 * np.power(x,3) - 10*x -3\\\n",
    "\n",
    "\n",
    "# x = np.linspace(-3, 3, 50)\n",
    "# plt.plot(x,f(x))\n",
    "# plt.plot(a,'o')\n",
    "# plt.xlim(-3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model theoretical whiff frequency as a gamma distribution with a scale factor that increases with distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiff_freq_from_distance(distance, N):\n",
    "    scale = 1 + 4*np.log(distance+1)\n",
    "    G = scipy.stats.gamma(2, 0, scale)\n",
    "    raw_whiff_frequencies = G.rvs(N)\n",
    "    return raw_whiff_frequencies\n",
    "\n",
    "distance = np.logspace(-1, 3)\n",
    "\n",
    "whiff_freqs = []\n",
    "distances = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    wfs = whiff_freq_from_distance(d, N)\n",
    "    whiff_freqs.extend( wfs.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( np.array(distances), np.array(whiff_freqs), '.', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With increasing distance, concentration falls, whiffs harder to detect, so frequency should fall\n",
    "\n",
    "Model concentration effect as a normal distribution with a mean that falls according to an exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concentration_effect_from_distance(distance, N):\n",
    "    mean = scipy.stats.expon(0,40).pdf(d) / 0.025\n",
    "    concentration_effects = scipy.stats.norm(mean, 0.3).rvs(N)\n",
    "    concentration_effects[concentration_effects<0] = 0\n",
    "    return concentration_effects\n",
    "\n",
    "distance = np.logspace(-1, 3)\n",
    "\n",
    "conc_effects = []\n",
    "distances = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    ces = concentration_effect_from_distance(d, N)\n",
    "    conc_effects.extend( ces.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( np.array(distances), np.array(conc_effects), '.', alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model observed whiff frequency as a product of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiff_freqs = []\n",
    "distances = []\n",
    "concentrations = []\n",
    "for d in distance:\n",
    "    N = 100\n",
    "    \n",
    "    raw_whiff_frequencies = whiff_freq_from_distance(d, N)\n",
    "    concentration_effects = concentration_effect_from_distance(d, N)\n",
    "    \n",
    "    wfs = raw_whiff_frequencies*concentration_effects\n",
    "    wfs = wfs / 10\n",
    "    \n",
    "    whiff_freqs.extend( wfs.tolist() )\n",
    "    distances.extend(N*[d])\n",
    "    concentrations.extend(concentration_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot( np.array(distances), np.array(whiff_freqs), '.', alpha=0.3)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(1e-1, 1e2)\n",
    "ax.set_ylim(0, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'distance': distances, \n",
    "                       'whiff_freq': whiff_freqs, \n",
    "                       'concentration': concentrations,\n",
    "                       'type': [0]*len(whiff_freqs)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
